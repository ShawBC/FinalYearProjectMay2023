# -*- coding: utf-8 -*-
"""Pytorch_TL_Exp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lNUdVKY7o3Ybeq8c-TmewPeCQH7mr1G8

# Introduction

![Example of Transfer Learning with YAMNet](Images\TransferLearningWithPretrainedAudioNetworksExample_01.png)

https://www.featureform.com/post/the-definitive-guide-to-embeddings

https://www.pinecone.io/learn/vector-embeddings/

https://vaclavkosar.com/ml/Embeddings-in-Machine-Learning-Explained

Audio recordings can be transformed into vectors using image embedding transformations over the audio frequencies visual representation (e.g., using its Spectrogram).

Uses frames that are 0.96 seconds long and extracts one frame every 0.48 seconds.

Accepts a 1-D float32 Tesnor or NumPy array -> single-channel 16kHz samples in range [-1.0, +1.0]

Returns 3 outputs -> class scores, embeddings and log mel spectrogram

https://tfhub.dev/google/yamnet/1

https://developers.google.com/codelabs/tflite-audio-classification-custom-model-android#2 -> Look at this for mobile app deployment

https://stackoverflow.com/questions/37999150/how-to-split-a-wav-file-into-multiple-wav-files


    An embedding is a mapping from discrete objects, such as words, to vectors of real numbers.

    The individual dimensions in these vectors typically have no inherent meaning. Instead, itâ€™s the overall patterns of location and distance between vectors that machine learning takes advantage of.

## Glossary

YAMNet -> pre-trained neural network that employs the Mobilenet_v1 depthwise-seperable convolution architecture

### Pre-processing - If Google Colab

I wil be using Tensorflows ***tf.keras.utils.audio_dataset_from_directory*** feature to create dataset labels and import them into my program.

In order to do this effectively I will need subdirectories of each chord along with its chord type. 

I will then use a python segment to extract 2 seconds of audio from each file and place them in their relevant folders. 

Modified Code from: https://stackoverflow.com/questions/37999150/how-to-split-a-wav-file-into-multiple-wav-files

https://stackoverflow.com/questions/36799902/how-to-splice-an-audio-file-wav-format-into-1-sec-splices-in-python
"""

from google.colab import files
uploaded = files.upload()

import io 
guitar_chords_dataframe = pd.read_excel(io.BytesIO(uploaded['CSV_Audio_Split_Guitar.xlsx']))

pip install pydub

import numpy as np

csv_labels = guitar_chords_dataframe['Chord'].to_numpy()

csv_labels

import pydub
from pydub import AudioSegment
from pydub.utils import make_chunks
import pandas as pd

class SplitWavChords():
  def __init__(self, filename, 
               chunk_length,
               guitar_csv_labels):
    self.guitar_csv_labels = guitar_csv_labels
    self.filename = filename
    self.chunk_length = chunk_length
    self.audio =  AudioSegment.from_wav(self.filename)
    self.chunks = make_chunks(self.audio, self.chunk_length)

  def multiple_splits(self):

    for i, chunk in enumerate(self.chunks):
      chunk_name = f"{self.guitar_csv_labels[i]}_{i}"
      chunk.export(chunk_name, format="wav")

files  = 'ableton_live_guitar_Campfire.wav'

split_wav = SplitWavChords(files, 2000,csv_labels)
split_wav.multiple_splits()

"""### Pre-processing - If Local


I wil be using Tensorflows ***tf.keras.utils.audio_dataset_from_directory*** feature to create dataset labels and import them into my program.

In order to do this effectively I will need subdirectories of each chord along with its chord type. 

I will then use a python segment to extract 2 seconds of audio from each file and place them in their relevant folders. 

Modified Code from: https://stackoverflow.com/questions/37999150/how-to-split-a-wav-file-into-multiple-wav-files

https://stackoverflow.com/questions/36799902/how-to-splice-an-audio-file-wav-format-into-1-sec-splices-in-python

"""

from google.colab import files
uploaded = files.upload()

import io 
guitar_chords_dataframe = pd.read_excel(io.BytesIO(uploaded['CSV_Audio_Split_Guitar.xlsx']))

pip install pydub

import numpy as np

csv_labels = guitar_chords_dataframe['Chord'].to_numpy()

csv_labels

import pydub
from pydub import AudioSegment
from pydub.utils import make_chunks
import pandas as pd

class SplitWavChords():
  def __init__(self, filename, 
               chunk_length,
               guitar_csv_labels):
    self.guitar_csv_labels = guitar_csv_labels
    self.filename = filename
    self.chunk_length = chunk_length
    self.audio =  AudioSegment.from_wav(self.filename)
    self.chunks = make_chunks(self.audio, self.chunk_length)

  def multiple_splits(self):

    for i, chunk in enumerate(self.chunks):
      chunk_name = f"{self.guitar_csv_labels[i]}_{i}"
      chunk.export(chunk_name, format="wav")

files  = 'ableton_live_guitar_Campfire.wav'

split_wav = SplitWavChords(files, 2000,csv_labels)
split_wav.multiple_splits()

"""### Dataset
The dataset I have chosen to use is the **IDMT-SMT-CHORDS** dataset for prediciting chords.

For prediciting progressions (or at least suggesting) I am using 
**IDMT-SMT-CHORD-SEQUENCES**.

Both datasets can be found @ :

https://www.idmt.fraunhofer.de/en/publications/datasets/chord-sequences.html

https://www.idmt.fraunhofer.de/en/publications/datasets/chords.html

---

### Import TensorFlow and other libraries
"""

!pip install -q "tensorflow==2.11.*"
!pip install -q "tensorflow_io==0.28.*"

import os

from IPython import display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_io as tfio

"""### Loading YAMNet from TensorFlow Hub

https://www.tensorflow.org/hub/tutorials/yamnet

"""

yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(yamnet_model_handle)

testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',
                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',
                                                cache_dir='./',
                                                cache_subdir='test_data')

print(testing_wav_file_name)

# Utility functions for loading audio files and making sure the sample rate is correct.

@tf.function
def load_wav_16k_mono(filename):
    """ Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. """
    file_contents = tf.io.read_file(filename)
    wav, sample_rate = tf.audio.decode_wav(
          file_contents,
          desired_channels=1)
    wav = tf.squeeze(wav, axis=-1)
    sample_rate = tf.cast(sample_rate, dtype=tf.int64)
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)
    return wav

testing_wav_data =  load_wav_16k_mono(testing_wav_file_name)

_ = plt.plot(testing_wav_data)

# Play the audio file.
display.Audio(testing_wav_data, rate=16000)

"""### Load the class mapping

Important to load class names that YAMNet is able to recognise, the ,mapping file is present at

> yammnet_model.class_map_path()




"""

class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8'
)
class_names =list(pd.read_csv(class_map_path)['display_name'])

for name in class_names[:20]:
  print(name)
print('...')

"""### Run inference
 YAMNet provides frame-level class_scores (i.e., 521 scores for every frame).

 In order to determine clip-level predicitions, the scores can be aggregated per-class across frames (e.g., using mean or max aggregation). scores_np.mean(axis=0)

 To find the top scored class at the clip level, take the maximum of the 521 aggregated scores.

 https://stackoverflow.com/questions/34236252/what-is-the-difference-between-np-mean-and-tf-reduce-mean
"""

scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
class_scores = tf.reduce_mean(scores, axis=0)
top_class = tf.math.argmax(class_scores)
inferred_class = class_names[top_class]

print(f'The main sound is: {inferred_class}')
print(f'The embeddings shape: {embeddings.shape}')

"""ESC-50 Dataset

https://github.com/karolpiczak/ESC-50#repository-content

Labelled collection of 2,000 five-second long environmental audio recordings. the dataset consists of 50 classes, with 40 examples per class.

Download the dataset and extract it.
"""

_ = tf.keras.utils.get_file('esc-50.zip',
                        'https://github.com/karoldvl/ESC-50/archive/master.zip',
                        cache_dir='./',
                        cache_subdir='datasets',
                        extract=True)

"""### Explore the data

The metadata for each file is specified in the csv file at ./datasets/ESC-50-master/meta/esc50.csv

and all the audio files are in ./datasets/ESC-50-master/audio/

You will create a pandas DataFrame with the mapping and use that to have a clearer view of the data.
"""

esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'
base_data_path = './datasets/ESC-50-master/audio/'

pd_data = pd.read_csv(esc50_csv)
print(pd_data)

"""### Filter the data

Now that the data is stored in the `DataFrame`, apply some transformations:

I will be using hen and sheep -- original tutorial uses dogs and cats

- Filter out rows and use only the selected classes - `hen` and `sheep`. If you want to use any other classes, this is where you can choose them.
- Amend the filename to have the full path. This will make loading easier later.
- Change targets to be within a specific range. In this example, `hen` will become `0`, and `sheep` will become `1` instead of its original value of `6` and `8`.


"""

my_classes = ['hen','sheep']
map_class_to_id = {'hen':0, 'sheep':1}

# Only grab the data we need from the hen and sheep catergories
filtered_pd = pd_data[pd_data.category.isin(my_classes)]

class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])
filtered_pd = filtered_pd.assign(target=class_id)

full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))
filtered_pd = filtered_pd.assign(filename=full_path)


filtered_pd.head(10)

"""### Load the audio files and retrieve embeddings

Here you'll apply the `load_wav_16k_mono` and prepare the WAV data for the model.

When extracting embeddings from the WAV data, you get an array of shape `(N, 1024)`  where `N` is the number of frames that YAMNet found (one for every 0.48 seconds of audio).

Your model will use each frame as one input. Therefore, you need to create a new column that has one frame per row. You also need to expand the labels and the `fold` column to proper reflect these new rows.

The expanded `fold` column keeps the original values. You cannot mix frames because, when performing the splits, you might end up having parts of the same audio on different splits, which would make your validation and test steps less effective.
"""

filenames = filtered_pd['filename']
targets = filtered_pd['target']
folds = filtered_pd['fold']


main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))
main_ds.element_spec

def load_wav_for_map(filename, label, fold):
  return load_wav_16k_mono(filename), label, fold

main_ds = main_ds.map(load_wav_for_map)
main_ds.element_spec

# applies the embedding extraction model to a wav data
def extract_embedding(wav_data, label, fold):
  ''' run YAMNet to extract embedding from the wav data '''
  scores, embeddings, spectrogram = yamnet_model(wav_data)
  num_embeddings = tf.shape(embeddings)[0]
  return (embeddings,
            tf.repeat(label, num_embeddings),
            tf.repeat(fold, num_embeddings))

# extract embedding
main_ds = main_ds.map(extract_embedding).unbatch()
main_ds.element_spec

"""### Split the data

You will use the `fold` column to split the dataset into train, validation and test sets.

ESC-50 is arranged into five uniformly-sized cross-validation `fold`s, such that clips from the same original source are always in the same `fold` - find out more in the [ESC: Dataset for Environmental Sound Classification](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf) paper.

The last step is to remove the `fold` column from the dataset since you're not going to use it during training.

"""

cached_ds = main_ds.cache()
train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)
val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)
test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)

# remove the folds column now that it's not needed anymore
remove_fold_column = lambda embedding, label, fold: (embedding, label)

train_ds = train_ds.map(remove_fold_column)
val_ds = val_ds.map(remove_fold_column)
test_ds = test_ds.map(remove_fold_column)

train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)

"""### Create Model

"""

my_model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,
                          name='input_embedding'),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(len(my_classes))
], name='my_model')

my_model.summary()

my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 optimizer="adam",
                 metrics=['accuracy'])

callback = tf.keras.callbacks.EarlyStopping(monitor='loss',
                                            patience=3,
                                            restore_best_weights=True)

history = my_model.fit(train_ds,
                       epochs=20,
                       validation_data=val_ds,
                       callbacks=callback)

loss, accuracy = my_model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

scores, embeddings, spectrogram = yamnet_model(testing_wav_data)
result = my_model(embeddings).numpy()

inferred_class = my_classes[result.mean(axis=0).argmax()]
print(f'The main sound is: {inferred_class}')